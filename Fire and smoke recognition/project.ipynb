{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8efc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6bf2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path the video\n",
    "vidcap = cv2.VideoCapture(r\"D:\\LBP-shortcut\\data\\xy.mp4\")\n",
    "success, image = vidcap.read()\n",
    "count = 0\n",
    "while vidcap.isOpened():\n",
    "    success, image = vidcap.read()\n",
    "    cv2.imwrite(r\"D:\\LBP-shortcut\\data\\Data\\fire\\%d.png\" % count, image)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8a4064",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path the video\n",
    "# vidcap1 = cv2.VideoCapture(r\"D:\\LBP-shortcut\\data\\no-fire.mp4\")\n",
    "# success, image1 = vidcap1.read()\n",
    "# count1 = 0\n",
    "# while vidcap1.isOpened():\n",
    "#     success, image1 = vidcap1.read()\n",
    "#     cv2.imwrite(r\"D:\\LBP-shortcut\\data\\Data\\no-fire\\%d.png\" % count1, image1)\n",
    "#     count1+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d1a117d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "training_datagenarator= ImageDataGenerator(rescale=1./255,horizontal_flip=True,\n",
    "    vertical_flip=True,shear_range=0.2,\n",
    "    zoom_range=0.2,width_shift_range=0.2,\n",
    "    height_shift_range=0.2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f89b99aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1470 images belonging to 2 classes.\n",
      "Found 163 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train=training_datagenarator.flow_from_directory(r'D:\\LBP-shortcut\\data\\Data',\n",
    "                                                target_size=(224, 224),color_mode='rgb',\n",
    "                                       class_mode='binary', batch_size=batch_size,subset='training')\n",
    "\n",
    "validation=training_datagenarator.flow_from_directory(r'D:\\LBP-shortcut\\data\\Data',\n",
    "                                                target_size=(224, 224),color_mode='rgb',\n",
    "                                       class_mode='binary', batch_size=batch_size,subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0a420538",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "# adding first layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=[224,224,3]))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "# adding second layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "# adding third layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "# Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "cnn.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "\n",
    "# Output layers\n",
    "cnn.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0f2e0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e615d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "cnn=load_model(r'D:\\LBP-shortcut\\data\\model\\fire.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9d54a177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import imgaug as ia\n",
    "from imgaug.augmentables.bbs import BoundingBox\n",
    "import skvideo.io\n",
    "import glob\n",
    "from moviepy.editor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "56c13e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video paperFire.mp4.\n",
      "Moviepy - Writing video paperFire.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready paperFire.mp4\n"
     ]
    }
   ],
   "source": [
    "# image_for_testing='D:/LBP-shortcut/data/Data/fire'\n",
    "i=0\n",
    "Image = []\n",
    "for i in range(count):\n",
    "\n",
    "    image_for_testing='D:/LBP-shortcut/data/Data/fire'+\"/\"+str(i)+\".png\"\n",
    "\n",
    "    \n",
    "    img = cv2.imread(str(image_for_testing))\n",
    "\n",
    "    # convert to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # threshold\n",
    "    thresh = cv2.threshold(gray,120,255,cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # get contours\n",
    "    result = img.copy()\n",
    "    contours = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "    for cntr in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cntr)\n",
    "        cv2.rectangle(result, (x, y), (x+w, y+h), (0, 0, 255), 2)\n",
    "\n",
    " \n",
    "    # save resulting image\n",
    "    cv2.imwrite('Fire-Images/fire'+str(i)+'.jpg',result)\n",
    "\n",
    "\n",
    "    Image = Image + ['Fire-Images/fire'+str(i)+'.jpg']\n",
    "\n",
    "\n",
    "\n",
    "clips = []\n",
    "for i in range(len(Image)):\n",
    "    clip =  ImageClip(Image[i]).set_duration(2)\n",
    "    clips.append(clip)\n",
    "\n",
    "video_clip = concatenate_videoclips(clips, method='compose')\n",
    "video_clip = video_clip.speedx(40)\n",
    "video_clip.write_videofile(\"paperFire.mp4\", fps=60, remove_temp=True, codec=\"libx264\", audio_codec=\"aac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954444b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
